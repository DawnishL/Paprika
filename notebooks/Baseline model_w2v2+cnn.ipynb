{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2548e0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import soundfile as sf\n",
    "import audiomentations\n",
    "print(audiomentations.__version__)\n",
    "from audiomentations import Compose, AddGaussianNoise, AddBackgroundNoise, TimeStretch, PitchShift, ClippingDistortion, RoomSimulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000f2282",
   "metadata": {},
   "source": [
    "# RAVDESS Dataset Organization and Label Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc28476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_ravdess(\n",
    "    data_path=\"/Audio_Speech_Actors_01-24\",\n",
    "    output_path=\"/Data/archive_processed\"\n",
    "):\n",
    "    emotion_map = {\n",
    "        '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n",
    "        '05': 'angry', '06': 'fear', '07': 'disgust', '08': 'surprise'\n",
    "    }\n",
    "    intensity_map = {'01': 'normal', '02': 'strong'}\n",
    "    statement_map = {\n",
    "        '01': 'Kids are talking by the door',\n",
    "        '02': 'Dogs are sitting by the door'\n",
    "    }\n",
    "    metadata = []\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for actor_dir in tqdm(sorted(os.listdir(data_path)), desc=\"Actors\"):\n",
    "        if not actor_dir.startswith('Actor_'):\n",
    "            continue\n",
    "        actor_path = os.path.join(data_path, actor_dir)\n",
    "        if not os.path.isdir(actor_path): continue\n",
    "        actor_id = actor_dir.split('_')[1]\n",
    "        gender = 'male' if int(actor_id) % 2 == 1 else 'female'\n",
    "        for filename in sorted(os.listdir(actor_path)):\n",
    "            if not filename.endswith('.wav'): continue\n",
    "            parts = filename.split('-')\n",
    "            if len(parts) != 7: continue\n",
    "            modality, vocal_channel, emo_code, inten_code, stmt_code, rep, actor = parts\n",
    "            if modality != '03' or emo_code not in emotion_map: continue\n",
    "            emotion = emotion_map[emo_code]\n",
    "            intensity = intensity_map[inten_code]\n",
    "            statement = statement_map.get(stmt_code, f\"statement_{stmt_code}\")\n",
    "            new_filename = (\n",
    "                f\"{emotion}_{intensity}_{gender}_actor{actor_id}_stmt{stmt_code}_rep{rep}.wav\"\n",
    "            )\n",
    "            src_path = os.path.join(actor_path, filename)\n",
    "            dest_path = os.path.join(output_path, new_filename)\n",
    "            shutil.copy2(src_path, dest_path)\n",
    "            metadata.append({\n",
    "                'original_filename': filename,\n",
    "                'processed_filename': new_filename,\n",
    "                'emotion': emotion,\n",
    "                'intensity': intensity,\n",
    "                'gender': gender,\n",
    "                'actor_id': actor_id,\n",
    "                'statement': statement,\n",
    "                'processed_path': dest_path,\n",
    "            })\n",
    "    df = pd.DataFrame(metadata)\n",
    "    df.to_csv(os.path.join(output_path, \"complete_metadata.csv\"), index=False)\n",
    "    print(f\"Meta data generated into {os.path.join(output_path, 'complete_metadata.csv')}, number of samples: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "df = organize_ravdess(\n",
    "    data_path=\"/Audio_Speech_Actors_01-24\",\n",
    "    output_path=\"/Data/archive_processed\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62620702",
   "metadata": {},
   "source": [
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d390b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "LE_PATH = \"/Data/label_encoders\"\n",
    "os.makedirs(LE_PATH, exist_ok=True)\n",
    "\n",
    "meta = pd.read_csv(\"/Data/archive_processed/complete_metadata.csv\")\n",
    "for col in [\"emotion\", \"intensity\", \"gender\"]:\n",
    "    le = LabelEncoder()\n",
    "    meta[col] = le.fit_transform(meta[col])\n",
    "    joblib.dump(le, os.path.join(LE_PATH, f\"le_{col}.pkl\"))\n",
    "meta.to_csv(\"/Data/archive_processed/complete_metadata_encoded.csv\", index=False)\n",
    "print(\"Label encoding completed. The encoded metadata has been saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8da21bf",
   "metadata": {},
   "source": [
    "# Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f396308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(\"/Data/archive_processed/complete_metadata_encoded.csv\")\n",
    "train_df, val_df = train_test_split(meta, stratify=meta[\"emotion\"], test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e87170",
   "metadata": {},
   "source": [
    "# Data Augmentation & Virtual Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3580c2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import Compose, AddGaussianNoise, AddBackgroundNoise, TimeStretch, PitchShift, \\\n",
    "    ClippingDistortion, RoomSimulator, PolarityInversion, Gain, BandPassFilter, BandStopFilter\n",
    "\n",
    "NOISE_DIR = \"/ESC-50-master\"\n",
    "\n",
    "def get_stronger_audio_augmentor():\n",
    "    return Compose([\n",
    "        PolarityInversion(p=0.2),\n",
    "        Gain(min_gain_db=-6, max_gain_db=6, p=0.3),\n",
    "        AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.008, p=0.4),\n",
    "        AddBackgroundNoise(sounds_path=NOISE_DIR, min_snr_db=3, max_snr_db=18, p=0.4),\n",
    "        TimeStretch(min_rate=0.95, max_rate=1.05, p=0.20),\n",
    "        PitchShift(min_semitones=-2, max_semitones=2, p=0.20),\n",
    "        BandPassFilter(min_center_freq=200.0, max_center_freq=4000.0, p=0.10),\n",
    "        BandStopFilter(min_center_freq=200.0, max_center_freq=4000.0, p=0.10),\n",
    "        ClippingDistortion(min_percentile_threshold=0, max_percentile_threshold=7, p=0.15),\n",
    "        RoomSimulator(p=0.15)\n",
    "    ])\n",
    "\n",
    "class SERDatasetAugmentedVirtual(Dataset):\n",
    "    def __init__(self, df, augment=True, sample_rate=16000, target_duration=3, n_aug_per_sample=3):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.augment = augment\n",
    "        self.sample_rate = sample_rate\n",
    "        self.target_samples = int(target_duration * sample_rate)\n",
    "        self.n_aug_per_sample = n_aug_per_sample if augment else 1\n",
    "        self.augmentor = get_stronger_audio_augmentor() if augment else None\n",
    "\n",
    "        self.virtual_index = []\n",
    "        for idx in range(len(self.df)):\n",
    "            for aug_idx in range(self.n_aug_per_sample):\n",
    "                self.virtual_index.append((idx, aug_idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.virtual_index)\n",
    "\n",
    "    def __getitem__(self, vidx):\n",
    "        idx, aug_idx = self.virtual_index[vidx]\n",
    "        row = self.df.iloc[idx]\n",
    "        audio, sr = sf.read(row[\"processed_path\"])\n",
    "        if sr != self.sample_rate:\n",
    "            import librosa\n",
    "            audio = librosa.resample(audio, orig_sr=sr, target_sr=self.sample_rate)\n",
    "        if audio.ndim == 2:\n",
    "            audio = audio.mean(axis=1)\n",
    "        if len(audio) < self.target_samples:\n",
    "            audio = np.pad(audio, (0, self.target_samples - len(audio)))\n",
    "        elif len(audio) > self.target_samples:\n",
    "            audio = audio[:self.target_samples]\n",
    "        assert len(audio) == self.target_samples, f\"{row['processed_path']} length error: {len(audio)}, expected {self.target_samples}\"\n",
    "        if self.augmentor and aug_idx > 0:\n",
    "            audio = self.augmentor(samples=audio, sample_rate=self.sample_rate)\n",
    "            if len(audio) < self.target_samples:\n",
    "                audio = np.pad(audio, (0, self.target_samples - len(audio)))\n",
    "            elif len(audio) > self.target_samples:\n",
    "                audio = audio[:self.target_samples]\n",
    "            assert len(audio) == self.target_samples, f\"{row['processed_path']} length error after augmentation{len(audio)}, expected {self.target_samples}\"\n",
    "        labels = (row[\"emotion\"], row[\"intensity\"], row[\"gender\"])\n",
    "        return audio.astype(np.float32), labels\n",
    "SAMPLE_RATE = 16000\n",
    "train_set = SERDatasetAugmentedVirtual(train_df, augment=True, sample_rate=SAMPLE_RATE, target_duration=3, n_aug_per_sample=4)\n",
    "val_set   = SERDatasetAugmentedVirtual(val_df,   augment=False, sample_rate=SAMPLE_RATE, target_duration=3, n_aug_per_sample=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f6bc41",
   "metadata": {},
   "source": [
    "# Wav2Vec2 Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f155bcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"facebook/wav2vec2-base\"\n",
    "SAMPLE_RATE = 16000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "w2v2_model = Wav2Vec2Model.from_pretrained(MODEL_NAME).to(device).eval()\n",
    "\n",
    "def collate_fn(batch):\n",
    "    audios, labels = zip(*batch)\n",
    "    audios = np.stack(audios)\n",
    "    labels = np.array(labels, dtype=np.int64)\n",
    "    return torch.tensor(audios, dtype=torch.float32), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "def extract_w2v2_features_seq(dataset):\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "    features, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_audios, batch_labels in loader:\n",
    "            assert batch_audios.shape[1] == 3 * SAMPLE_RATE, f\"Audio length {batch_audios.shape[1]} doesn't equal 3-second target {3 * SAMPLE_RATE}\"\n",
    "            inputs = processor(\n",
    "                batch_audios.numpy(),\n",
    "                sampling_rate=SAMPLE_RATE,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\"\n",
    "            )\n",
    "            outs = w2v2_model(input_values=inputs.input_values.to(device))\n",
    "            emb_seq = outs.last_hidden_state.cpu().numpy()\n",
    "            features.append(emb_seq)\n",
    "            all_labels.append(batch_labels.numpy())\n",
    "\n",
    "    X = np.concatenate(features, axis=0)\n",
    "    y = np.concatenate(all_labels, axis=0)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "X_train, y_train = extract_w2v2_features_seq(train_set)\n",
    "X_val, y_val = extract_w2v2_features_seq(val_set)\n",
    "print(\"Wav2Vec2 frame sequence feature shape:\", X_train.shape, X_val.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b82e99",
   "metadata": {},
   "source": [
    "# Definition & Training of the Multi-task CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac8be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class DeeperMultiTaskCNNClassifier(nn.Module):\n",
    "    def __init__(self, emb_dim, n_emotion, n_intensity, n_gender, \n",
    "                 cnn_channels=128, kernel_size=5, hidden=256, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(emb_dim, cnn_channels, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.BatchNorm1d(cnn_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Conv1d(cnn_channels, cnn_channels*2, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.BatchNorm1d(cnn_channels*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Conv1d(cnn_channels*2, cnn_channels*2, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.BatchNorm1d(cnn_channels*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Conv1d(cnn_channels*2, cnn_channels, kernel_size=1),\n",
    "            nn.BatchNorm1d(cnn_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(cnn_channels, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.head_emotion   = nn.Linear(hidden, n_emotion)\n",
    "        self.head_intensity = nn.Linear(hidden, n_intensity)\n",
    "        self.head_gender    = nn.Linear(hidden, n_gender)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        cnn_out = self.cnn(x)\n",
    "        feat = self.fc(cnn_out)\n",
    "        return self.head_emotion(feat), self.head_intensity(feat), self.head_gender(feat)\n",
    "\n",
    "n_emotion = 8\n",
    "n_intensity = 2\n",
    "n_gender = 2\n",
    "model = DeeperMultiTaskCNNClassifier(\n",
    "    emb_dim=X_train.shape[2],\n",
    "    n_emotion=n_emotion,\n",
    "    n_intensity=n_intensity,\n",
    "    n_gender=n_gender\n",
    ").to(device)\n",
    "\n",
    "\n",
    "print(\"Wav2Vec2 frame sequence feature shape:\", X_train.shape, X_val.shape)\n",
    "\n",
    "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_e = torch.tensor(y_train[:, 0], dtype=torch.long)\n",
    "y_train_i = torch.tensor(y_train[:, 1], dtype=torch.long)\n",
    "y_train_g = torch.tensor(y_train[:, 2], dtype=torch.long)\n",
    "\n",
    "X_val_torch = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_e = torch.tensor(y_val[:, 0], dtype=torch.long)\n",
    "y_val_i = torch.tensor(y_val[:, 1], dtype=torch.long)\n",
    "y_val_g = torch.tensor(y_val[:, 2], dtype=torch.long)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3\n",
    ")\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "patience = 7 \n",
    "\n",
    "best_val_loss = np.inf\n",
    "best_model_state = None\n",
    "wait = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_acc_emotion = []\n",
    "val_acc_intensity = []\n",
    "val_acc_gender = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    idx = np.random.permutation(len(X_train_torch))\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(0, len(idx), BATCH_SIZE):\n",
    "        batch = idx[i:i+BATCH_SIZE]\n",
    "        xb = X_train_torch[batch].to(device)\n",
    "        yb_e = y_train_e[batch].to(device)\n",
    "        yb_i = y_train_i[batch].to(device)\n",
    "        yb_g = y_train_g[batch].to(device)\n",
    "        out_e, out_i, out_g = model(xb)\n",
    "        loss = 2.0 * loss_fn(out_e, yb_e) + 1.0 * loss_fn(out_i, yb_i) + 0.5 * loss_fn(out_g, yb_g) \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * len(batch)\n",
    "    avg_loss = total_loss / len(X_train_torch)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ve, vi, vg = model(X_val_torch.to(device))\n",
    "        pred_e = ve.argmax(1).cpu().numpy()\n",
    "        pred_i = vi.argmax(1).cpu().numpy()\n",
    "        pred_g = vg.argmax(1).cpu().numpy()\n",
    "        \n",
    "        acc_e = np.mean(pred_e == y_val[:, 0])\n",
    "        acc_i = np.mean(pred_i == y_val[:, 1])\n",
    "        acc_g = np.mean(pred_g == y_val[:, 2])\n",
    "        \n",
    "        val_loss = (\n",
    "            2.0 * loss_fn(ve, y_val_e.to(device)) +\n",
    "            1.0 * loss_fn(vi, y_val_i.to(device)) +\n",
    "            0.5 * loss_fn(vg, y_val_g.to(device))\n",
    "        ).item() / 3.5\n",
    "\n",
    "    val_losses.append(val_loss)\n",
    "    val_acc_emotion.append(acc_e)\n",
    "    val_acc_intensity.append(acc_i)\n",
    "    val_acc_gender.append(acc_g)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "          f\"Loss: {avg_loss:.4f} | \"\n",
    "          f\"Val_Loss: {val_loss:.4f} | \"\n",
    "          f\"Val_Emo: {acc_e*100:.2f}% | \"\n",
    "          f\"Val_Inten: {acc_i*100:.2f}% | \"\n",
    "          f\"Val_Gend: {acc_g*100:.2f}% | \"\n",
    "          f\"LR: {current_lr:.6f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}. Best val_loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "model.load_state_dict(best_model_state)\n",
    "torch.save(model.state_dict(), \"w2v2_multitask_cnn_XAI_best.pt\")\n",
    "print(\"Best model saved to w2v2_multitask_cnn_XAI_best.pt\")\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss Curve\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(val_acc_emotion, label=\"Val Acc Emotion\")\n",
    "plt.plot(val_acc_intensity, label=\"Val Acc Intensity\")\n",
    "plt.plot(val_acc_gender, label=\"Val Acc Gender\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Validation Accuracy Curves\")\n",
    "plt.show()\n",
    "\n",
    "OUTPUT_DIR = \"/Data/w2v2/output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "np.save(os.path.join(OUTPUT_DIR, \"train_losses.npy\"), np.array(train_losses))\n",
    "np.save(os.path.join(OUTPUT_DIR, \"val_losses.npy\"), np.array(val_losses))\n",
    "np.save(os.path.join(OUTPUT_DIR, \"val_acc_emotion.npy\"), np.array(val_acc_emotion))\n",
    "np.save(os.path.join(OUTPUT_DIR, \"val_acc_intensity.npy\"), np.array(val_acc_intensity))\n",
    "np.save(os.path.join(OUTPUT_DIR, \"val_acc_gender.npy\"), np.array(val_acc_gender))\n",
    "\n",
    "print(f\"Training log saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f10b49c",
   "metadata": {},
   "source": [
    "# Classification Report & Audio Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daa9335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    ve, vi, vg = model(X_val_torch.to(device))\n",
    "    pred_e = ve.argmax(1).cpu().numpy()\n",
    "    pred_i = vi.argmax(1).cpu().numpy()\n",
    "    pred_g = vg.argmax(1).cpu().numpy()\n",
    "\n",
    "LE_PATH = \"/Data/label_encoders\"\n",
    "le_e = joblib.load(os.path.join(LE_PATH, \"le_emotion.pkl\"))\n",
    "le_i = joblib.load(os.path.join(LE_PATH, \"le_intensity.pkl\"))\n",
    "le_g = joblib.load(os.path.join(LE_PATH, \"le_gender.pkl\"))\n",
    "\n",
    "print(\"Emotion:\\n\", classification_report(y_val[:, 0], pred_e, target_names=le_e.classes_))\n",
    "print(\"Intensity:\\n\", classification_report(y_val[:, 1], pred_i, target_names=le_i.classes_))\n",
    "print(\"Gender:\\n\", classification_report(y_val[:, 2], pred_g, target_names=le_g.classes_))\n",
    "\n",
    "\n",
    "def predict_audio(path, le_e, le_i, le_g):\n",
    "    y, sr = sf.read(path)\n",
    "    if sr != SAMPLE_RATE:\n",
    "        import librosa\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=SAMPLE_RATE)\n",
    "    if y.ndim > 1: y = y.mean(axis=1)\n",
    "\n",
    "    inputs = processor(y, sampling_rate=SAMPLE_RATE, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        emb_seq = w2v2_model(input_values=inputs.input_values.to(device)).last_hidden_state.cpu().numpy()[0]\n",
    "        emb_seq = (emb_seq - emb_seq.mean()) / (emb_seq.std() + 1e-8)\n",
    "        single_x_tensor = torch.tensor(emb_seq, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        out_e, out_i, out_g = model(single_x_tensor)\n",
    "        pred_e = out_e.argmax(1).cpu().numpy()[0]\n",
    "        pred_i = out_i.argmax(1).cpu().numpy()[0]\n",
    "        pred_g = out_g.argmax(1).cpu().numpy()[0]\n",
    "    return {\n",
    "        \"emotion\": le_e.inverse_transform([pred_e])[0],\n",
    "        \"intensity\": le_i.inverse_transform([pred_i])[0],\n",
    "        \"gender\": le_g.inverse_transform([pred_g])[0]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "test_path = \"/Data/test/happy.wav\"\n",
    "result = predict_audio(test_path, le_e, le_i, le_g)\n",
    "print(\"Prediction:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307fc973",
   "metadata": {},
   "source": [
    "# XAI Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3fb4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from captum.attr import IntegratedGradients\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "label_map = {\n",
    "    0: 'neutral', 1: 'calm', 2: 'happy', 3: 'sad',\n",
    "    4: 'angry', 5: 'fear', 6: 'disgust', 7: 'surprise'\n",
    "}\n",
    "\n",
    "\n",
    "def model_forward_wrapper(input_tensor):\n",
    "    output_tuple = model(input_tensor)\n",
    "    return output_tuple[0]\n",
    "\n",
    "def analyze_with_ig(sample_idx, target_class_index, target_label_name):\n",
    "    input_tensor = X_val_torch[sample_idx:sample_idx+1].to(device)\n",
    "    \n",
    "    ig = IntegratedGradients(model_forward_wrapper)\n",
    "    \n",
    "    attributions, delta = ig.attribute(\n",
    "        input_tensor,\n",
    "        baselines=torch.zeros_like(input_tensor),\n",
    "        target=target_class_index,\n",
    "        return_convergence_delta=True\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(attributions[0].detach().cpu().numpy().T, aspect='auto', cmap='hot')\n",
    "    plt.colorbar()\n",
    "    plt.title(f'Integrated Gradients for Sample {sample_idx} (Target: {target_label_name})')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Embedding Dimension')\n",
    "    plt.show()\n",
    "\n",
    "happy_label = -1\n",
    "try:\n",
    "    happy_label = list(label_map.values()).index('happy')\n",
    "except ValueError:\n",
    "    print(\"Error: 'happy' is not in label_map.\")\n",
    "\n",
    "if happy_label != -1:\n",
    "    print(\"Using the current model to generate the latest predictions...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_outputs = model_forward_wrapper(X_val_torch.to(device))\n",
    "        pred_e = torch.argmax(all_outputs, dim=1).cpu().numpy()\n",
    "    print(\"Latest predictions generated.\")\n",
    "\n",
    "    happy_wrong_idx = [i for i in range(len(y_val)) if y_val[i, 0] == happy_label and pred_e[i] != happy_label]\n",
    "\n",
    "    print(f\"\\nTotal number of 'happy' samples misclassified: {len(happy_wrong_idx)}\")\n",
    "    print(f\"Indices of misclassified 'happy' samples: {happy_wrong_idx}\")\n",
    "\n",
    "    if happy_wrong_idx:\n",
    "        num_to_analyze = 7 \n",
    "        \n",
    "        for sample_idx in happy_wrong_idx[:num_to_analyze]:\n",
    "            print(f\"\\n==================== Analysis of Sample {sample_idx} ====================\")\n",
    "\n",
    "            true_class_index = int(y_val[sample_idx, 0])\n",
    "            true_label_name = label_map.get(true_class_index, \"Unknown\")\n",
    "            \n",
    "            predicted_class_index = int(pred_e[sample_idx])\n",
    "            predicted_label_name = label_map.get(predicted_class_index, \"Unknown\")\n",
    "\n",
    "            print(f\"True label: {true_label_name} ({true_class_index})\")\n",
    "            print(f\"Model prediction: {predicted_label_name} ({predicted_class_index})\")\n",
    "\n",
    "            print(\"\\n -> Generating attribution map (Target: happy)...\")\n",
    "            analyze_with_ig(\n",
    "                sample_idx=sample_idx,\n",
    "                target_class_index=true_class_index,\n",
    "                target_label_name=true_label_name\n",
    "            )\n",
    "\n",
    "            print(f\"\\n -> Generating attribution map (Target: {predicted_label_name})...\")\n",
    "            analyze_with_ig(\n",
    "                sample_idx=sample_idx,\n",
    "                target_class_index=predicted_class_index,\n",
    "                target_label_name=predicted_label_name\n",
    "            )\n",
    "            print(f\"==================== Analysis of Sample {sample_idx} completed ====================\\n\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nCongratulations! The model didn't misclassify any 'happy' samples. No analysis needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5115f93f",
   "metadata": {},
   "source": [
    "# Visualization & Performance Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35e14c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from PIL import Image\n",
    "\n",
    "VIS_DIR = \"/Data/w2v2/visualizations/w2v2_multitask_cnn_XAI\"\n",
    "EMB_DIR = \"/Data/XAI/w2v2\"\n",
    "LOG_DIR = \"/Data/w2v2/output\"\n",
    "\n",
    "\n",
    "os.makedirs(VIS_DIR, exist_ok=True)\n",
    "os.makedirs(EMB_DIR, exist_ok=True) \n",
    "os.makedirs(LOG_DIR, exist_ok=True) \n",
    "\n",
    "assert 'X_train' in globals() and 'y_train' in globals()\n",
    "\n",
    "X = X_train \n",
    "y_all = y_train \n",
    "\n",
    "import pandas as pd\n",
    "df_info = pd.DataFrame({\n",
    "    \"emotion\": y_all[:, 0],\n",
    "    \"intensity\": y_all[:, 1],\n",
    "    \"gender\": y_all[:, 2],\n",
    "})\n",
    "y_true = df_info[\"emotion\"].values\n",
    "\n",
    "np.save(os.path.join(EMB_DIR, \"emb_train_X.npy\"), X)\n",
    "df_info.to_csv(os.path.join(EMB_DIR, \"emb_train_info.csv\"), index=False)\n",
    "print(\"Automatically generated emb_train_X.npy and emb_train_info.csv files.\")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i, col in enumerate([\"emotion\", \"intensity\", \"gender\"]):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    df_info[col].value_counts().sort_index().plot(kind='bar')\n",
    "    plt.title(f\"{col.capitalize()} Distribution\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "label_path = os.path.join(VIS_DIR, \"label_distribution_summary_XAI.png\")\n",
    "plt.savefig(label_path, dpi=300)\n",
    "plt.close()\n",
    "\n",
    "if X.ndim == 3:\n",
    "    X = X.mean(axis=1)\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "X_emb = tsne.fit_transform(X)\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_emb[:, 0], X_emb[:, 1], c=y_true, cmap=\"tab10\", s=10, alpha=0.7)\n",
    "plt.title(\"t-SNE Projection of Embeddings\")\n",
    "plt.colorbar(scatter, label=\"Emotion\")\n",
    "tsne_path = os.path.join(VIS_DIR, \"tsne_emotion_projection_XAI.png\")\n",
    "plt.savefig(tsne_path, dpi=300)\n",
    "plt.close()\n",
    "\n",
    "loss_train = np.load(os.path.join(LOG_DIR, \"train_losses.npy\"))\n",
    "loss_val   = np.load(os.path.join(LOG_DIR, \"val_losses.npy\"))\n",
    "acc_e = np.load(os.path.join(LOG_DIR, \"val_acc_emotion.npy\"))\n",
    "acc_i = np.load(os.path.join(LOG_DIR, \"val_acc_intensity.npy\"))\n",
    "acc_g = np.load(os.path.join(LOG_DIR, \"val_acc_gender.npy\"))\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axs[0].plot(loss_train, label=\"Train Loss\")\n",
    "axs[0].plot(loss_val, label=\"Val Loss\")\n",
    "axs[0].set_title(\"Loss Curve\")\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(acc_e, label=\"Emotion Acc\")\n",
    "axs[1].plot(acc_i, label=\"Intensity Acc\")\n",
    "axs[1].plot(acc_g, label=\"Gender Acc\")\n",
    "axs[1].set_title(\"Validation Accuracy\")\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].set_ylabel(\"Accuracy\")\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "curve_path = os.path.join(VIS_DIR, \"training_curves_summary_XAI.png\")\n",
    "plt.savefig(curve_path, dpi=300)\n",
    "plt.close()\n",
    "\n",
    "img1 = Image.open(label_path).resize((600, 300))\n",
    "img2 = Image.open(tsne_path).resize((600, 300))\n",
    "img3 = Image.open(curve_path).resize((600, 300))\n",
    "combined_img = Image.new(\"RGB\", (1800, 300), color=(255, 255, 255))\n",
    "combined_img.paste(img1, (0, 0))\n",
    "combined_img.paste(img2, (600, 0))\n",
    "combined_img.paste(img3, (1200, 0))\n",
    "combined_path = os.path.join(VIS_DIR, \"summary_all_combined_XAI.png\")\n",
    "combined_img.save(combined_path)\n",
    "\n",
    "val_preds_path = os.path.join(LOG_DIR, \"val_preds_emotion.npy\")\n",
    "y_pred = np.load(val_preds_path) if os.path.exists(val_preds_path) else y_true\n",
    "cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y_true))\n",
    "plt.figure(figsize=(8, 6))\n",
    "disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "plt.title(\"Confusion Matrix - Emotion\")\n",
    "cm_path = os.path.join(VIS_DIR, \"summary_confusion_XAI.png\")\n",
    "plt.savefig(cm_path, dpi=300)\n",
    "plt.close()\n",
    "\n",
    "if X.ndim == 3:\n",
    "    sample_idx = 0\n",
    "    feat_sample = X[sample_idx]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(feat_sample.T, aspect='auto', origin='lower', cmap=\"viridis\")\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Feature Map of Sample 0\")\n",
    "    plt.xlabel(\"Time Frames\")\n",
    "    plt.ylabel(\"Feature Dimensions\")\n",
    "    fmap_path = os.path.join(VIS_DIR, \"summary_featmap_XAI.png\")\n",
    "    plt.savefig(fmap_path, dpi=300)\n",
    "    plt.close()\n",
    "else:\n",
    "    fmap_path = None\n",
    "\n",
    "report_text = classification_report(y_true, y_pred, output_dict=False)\n",
    "report_path = os.path.join(VIS_DIR, \"summary_clsreport_XAI.txt\")\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(report_text)\n",
    "\n",
    "print(\"Visualizations and reports generated:\")\n",
    "print(\"1. Label distribution:\", label_path)\n",
    "print(\"2. t-SNE projection:\", tsne_path)\n",
    "print(\"3. Training curves:\", curve_path)\n",
    "print(\"4. Combined image:\", combined_path)\n",
    "print(\"5. Confusion matrix:\", cm_path)\n",
    "print(\"6. Feature map:\", fmap_path)\n",
    "print(\"7. Classification report:\", report_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
